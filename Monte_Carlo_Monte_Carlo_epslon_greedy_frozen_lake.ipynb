{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Monte Carlo Control without Exploring Starts Frozen Lake: $\\epsilon$ (epsilon)-greedy policies</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will use Perform Monte Carlo Control without Exploring Starts using $\\epsilon$ (epsilon)-greedy policies on the BlackJack environment on open AI gym </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#UF\">Utility Functions and Imports</a></li>\n",
    "    <li><a href=\"#F\">Monte Carlo Control without Exploring Starts</a></li>\n",
    "    <li><a href=\"#FE\">Monte Carlo Prediction Experiments </a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda seaborn\n",
    " \n",
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"UF\">Utility Functions and Imports </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from deterministicfrozenlake import  FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 id=\"F\">Exploring Starts Function </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will select the maximum argument of its input,  with the probability of epsilon. Otherwise, it will randomly select the non greedy action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrawStateGrid():\n",
    "    def __init__(self,Environment,v=None):\n",
    "        state_number=4\n",
    "        self.state_name=[['S','F','F','F'],\n",
    "        ['F','H','F','H'],\n",
    "        ['F','F','F','H'],\n",
    "        ['H','F','F','G']]\n",
    "        self.numbber=[[0,1,2,3],\n",
    "        [4,5,6,7],\n",
    "        [8,9,10,11],\n",
    "        [12,13,14,15]]\n",
    "\n",
    "        self.state_action_state=np.zeros((Environment.nS,Environment.nA))\n",
    "        \n",
    "        \n",
    "        for state in range(Environment.nS):\n",
    "            for action in range(Environment.nA):\n",
    "                if len(Environment.P[state][action])>1:\n",
    "                    self.state_action_state[state,action]=Environment.P[state][action][1][1]\n",
    "                else: \n",
    "                       self.state_action_state[state,action]=Environment.P[state][action][0][1]\n",
    "        \n",
    "        major_ticks=[i+1 for i in range(state_number)]\n",
    "        minor_ticks=[i for i in range(state_number)]\n",
    "        self.fig,self.ax = plt.subplots()\n",
    "\n",
    "\n",
    "        self.ax.set_xlim([0,state_number])\n",
    "        self.ax.set_ylim([0,state_number])\n",
    "        self.ax.grid()\n",
    "        self.ax.set_xticks(major_ticks)\n",
    "        self.ax.set_xticks(minor_ticks, minor=True)\n",
    "        self.ax.set_yticks(major_ticks)\n",
    "        self.ax.set_yticks(minor_ticks, minor=True)\n",
    "        n=0 \n",
    "        for i,row in enumerate(self.state_name):\n",
    "            for j,val in enumerate(row): \n",
    "                self.ax.text(j,4-i-1,\"{}:{}\".format(val,str(n)),size=25)\n",
    "           \n",
    "                n+=1\n",
    "        \n",
    "    \n",
    "    def plot_state(self,start_state,end_state,action):\n",
    "\n",
    "        #start_state=0\n",
    "        #end_state=1\n",
    "        #action=0\n",
    "        #map states to position on table, top line on square subtract one and its the bottom line\n",
    "        fig2 = plt.figure()\n",
    "        y_top=[4,4,4,4,3,3,3,3,2,2,2,2,1,1,1,1]\n",
    "        \n",
    "        action_arrow=[(0.5,0),(0,-0.5),(-0.5,0),(0,0.5)]\n",
    "        \n",
    "        x_start=np.linspace(start_state%4,start_state%4+1,10)\n",
    "        self.ax.fill_between(x_start,y_top[start_state]-1,y_top[start_state],color='b')\n",
    "        \n",
    "        x_stop=np.linspace(end_state%4,end_state%4+1,10)\n",
    "\n",
    "        self.ax.fill_between( x_stop,y_top[end_state]-1,y_top[end_state],color='r')\n",
    "        \n",
    "     \n",
    "        self.ax.arrow(x_start[5], y_top[start_state]-0.5, action_arrow[action][0], action_arrow[action][1],head_width=0.05, head_length=0.1, fc='k', ec='k') \n",
    "        \n",
    "    def plot_policy(self,policy):\n",
    "   \n",
    "    \n",
    "        action_arrow=[(-0.5,0),(0,-0.5),(0.5,0),(0,0.5)]\n",
    "        y_top=[4,4,4,4,3,3,3,3,2,2,2,2,1,1,1,1]    \n",
    "        for state,action_set in enumerate(policy):\n",
    "            action=np.argmax(action_set)\n",
    "            if max(action_set)==1:\n",
    "                x_start=np.linspace(state%4,state%4+1,10)\n",
    "                self.ax.arrow(x_start[5], y_top[state]-0.5, action_arrow[action][0], action_arrow[action][1],head_width=0.05, head_length=0.1, fc='k', ec='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(z,epsilon=0.1):\n",
    "    \"\"\"\n",
    "    This function will select the maximum argument of its input, with the probability of epsilon. Otherwise, it will randomly select the non greedy action\n",
    "    Returns:\n",
    "    Args:\n",
    "    z:numpy array representing the action function \n",
    "    epsilon: probability of selecting the non-greedy action \n",
    "    policy:policy for blackjack if none a random  action will be selected \n",
    "    \"\"\"\n",
    "  \n",
    "    \n",
    "    argmax_=np.random.choice(np.where(z==z.max())[0])\n",
    "    \n",
    "    return argmax_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(action,epsilon=0.1,n_actions=4):\n",
    "    \n",
    "    number=np.random.rand(1)\n",
    "    \n",
    "    if number<1-epsilon:\n",
    "        return action\n",
    "    else:\n",
    "        action=np.random.randint(n_actions)  \n",
    "        return action \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `random_action` function will add noise to the action with a probability of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[0,1,2,3,0,0,0,0]\n",
    "for t in test:\n",
    "    print(\"a\",t)\n",
    "    print(\"a'\",random_action(t,epsilon=0.95,n_actions=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_one_hot(policy):\n",
    "    POLICY=np.zeros((16,4))\n",
    "\n",
    "    for i,action in  enumerate(policy):\n",
    "\n",
    "        POLICY[i,int(action)]=1\n",
    "\n",
    "    return POLICY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array data has its maximum value at index 0. We set epsilon to 0.75; as a result, it will only select the maximum value  25% of the time. Let's run the experiment 10 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see most of the samples are not the index with the largest values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines the optimal policy using the  $\\epsilon$ (epsilon)-greedy method. The input parameters are the discount factor, the number of episodes, epsilon value  and the open AI gym objects.  The class also Specifies if first-visit  and every-visit method. The output is the policy,value function and action function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_ES( environment,N_episodes=10000,epsilon=0.1, discount_factor=1,first_visit=True,theta=0):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy using the epsilon greedy method. The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects. The class also Specifies if first-visit and every-visit method. The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy: a dictionary of estimated policy for blackjack \n",
    "    V: a dictionary of estimated values for blackjack \n",
    "    Q: a dictionary of estimated action function\n",
    "    DELTA: list of deltas for each episode\n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    first_visit: select first-visit MC (Ture) and every-visit MC (False)\n",
    "    \"\"\"  \n",
    "    #a dictionary of estimated values for blackjack \n",
    "    V=defaultdict(float)\n",
    "    #a dictionary of estimated action function for blackjack\n",
    "    Q=defaultdict(float)\n",
    "    # number of visits to the action function \n",
    "    NumberVisitsValue= defaultdict(float)\n",
    "    # visits to action function\n",
    "    NumberVisits= defaultdict(float)\n",
    "    #dictionary  for policy \n",
    "    policy=defaultdict(float) \n",
    "    #number  of actions \n",
    "    number_actions=environment.action_space.n\n",
    "\n",
    "    DELTA=[] \n",
    "    \n",
    "    for i in range(N_episodes):\n",
    "        \n",
    "        delta=0\n",
    "        \n",
    "        #list that stores each state and reward for each episode     \n",
    "        episode=[]\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        state=environment.reset()   \n",
    "        #reward for the first state (only for frozen lake) \n",
    "        reward=0.0\n",
    "        #flag for end of episodes  \n",
    "        done=False\n",
    "        \n",
    "         #check if a policy for the state exists  \n",
    "        if isinstance(policy[state],np.int64):\n",
    "            #obtain action from policy\n",
    "            action=policy[state]\n",
    "            action=random_action(action,epsilon=0.1,n_actions=4)\n",
    "\n",
    "        else:\n",
    "            #if no policy for the state exists  select a random  action  \n",
    "            action=np.random.randint(number_actions)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #append firt state, reward and action\n",
    "        episode.append({'state':state , 'reward':reward,'action':action})\n",
    "        #Past states for signal visit  Monte Carlo \n",
    "        state_action=[(state,action)]\n",
    "        #enumerate for each episode \n",
    "        while not(done):\n",
    "\n",
    "                #take action and find next state, reward and check if the episode is  done (True)\n",
    "                (state, reward, done, prob) = environment.step(action)\n",
    "\n",
    "                #check if a policy for the state exists  \n",
    "                if isinstance(policy[state],np.int64):\n",
    "                    #obtain action from policy\n",
    "                    action=int(policy[state])\n",
    "                    action=random_action(action,epsilon=0.01,n_actions=4)\n",
    "\n",
    "                else:\n",
    "                     #if no policy for the state exists  select a random  action  \n",
    "                    action=np.random.randint(number_actions)\n",
    "                    \n",
    "                #add state reward and action to list \n",
    "                episode.append({'state':state , 'reward':reward,'action':action})\n",
    "                #add  states action this is for fist visit only \n",
    "                state_action.append((state,action))\n",
    "         #reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "        #append the state-action pairs to a list \n",
    "        state_action.reverse()\n",
    "\n",
    "\n",
    "        # determine the return\n",
    "        G=0\n",
    "        flag=0\n",
    "        for t,step in enumerate(episode):\n",
    "                \n",
    "                #check flag for first visit\n",
    "                #G=discount_factor*G+step['reward']\n",
    "                #if G==1 and flag==0 and t==0:\n",
    "                    #print(episode)\n",
    "                    #print(\"----------------------------------\")\n",
    "                    \n",
    "                #check flag for first visit\n",
    "              \n",
    "                if first_visit:\n",
    "                    #check if the state has been visited before \n",
    "                    \n",
    "                    if (step['state'],step['action']) not in state_action[t+1:]: \n",
    "\n",
    "                        #increment counter for action \n",
    "                        NumberVisits[step['state'],step['action']]+=1\n",
    "                        #increment counter for value function \n",
    "                        NumberVisitsValue[step['state']]+=1\n",
    "                        #if the action function value  does not exist, create an array  to store them \n",
    "                        if not(isinstance(Q[step['state']],np.ndarray) ):\n",
    "                            Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n",
    "                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                        \n",
    "                        v=V[step['state']]\n",
    "                        \n",
    "                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n",
    "                        ##update the policy to select the action fuciton argment with the largest value or randomly select a different action \n",
    "                        policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        \n",
    "                         \n",
    "                        #find max difference between all value functions per  iteration \n",
    "                        delta=max(delta,abs(v-V[step['state']]))\n",
    "                        \n",
    "                else:\n",
    "                         #increment counter for action \n",
    "                        NumberVisits[step['state'],step['action']]+=1\n",
    "                        #increment counter for value function \n",
    "                        NumberVisitsValue[step['state']]+=1\n",
    "                        #if the action function value  does not exist, create an array  to store them \n",
    "                        if not(isinstance(Q[step['state']],np.ndarray) ):\n",
    "                            Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n",
    "                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                        #old value of value function\n",
    "                        v=V[step['state']]\n",
    "                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n",
    "                        ##update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "                        policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        \n",
    "                        #find max difference between all value functions per  iteration \n",
    "                        delta=max(delta,abs(v-V[step['state']]))\n",
    "               \n",
    "            \n",
    "                G=discount_factor*G+step['reward']\n",
    "        \n",
    "        #Delta for each episode  \n",
    "        DELTA.append(delta)\n",
    "        if delta<theta:\n",
    "            break\n",
    "         \n",
    "                    \n",
    "\n",
    "    \n",
    "    return policy, V, Q,DELTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 id=\"FE\">Monte Carlo Prediction Experiments  </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s  run different iterations of and see how different  parameters such as number of episodes  and discount factor effects performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an openAI gym blackjack enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "environment=gym.make('FrozenLakeNotSlippery-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we use a random policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform  $\\epsilon$ -greedy method  with 1000 episodes and  $\\epsilon$=0.1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>LEFT = 0</p>\n",
    "<p>DOWN = 1</p>\n",
    "<p>RIGHT = 2</p>\n",
    "<p>UP = 3</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, V, Q,DELTA= monte_carlo_ES( environment,N_episodes=1000,epsilon=0.01, discount_factor=1,first_visit=True,theta=0)  \n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an `actions` array for purposes of plotting the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=np.zeros(16)\n",
    "for i,action in zip(policy.keys(),policy.values()):\n",
    "    actions[i]=action\n",
    "    \n",
    "policy_one_hot(actions)\n",
    "\n",
    "#print(Q)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>LEFT = 0</p>\n",
    "<p>DOWN = 1</p>\n",
    "<p>RIGHT = 2</p>\n",
    "<p>UP = 3</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy is shown on the grid as arrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=DrawStateGrid(environment.env,np.zeros(15))\n",
    "test.plot_policy(policy_one_hot(actions))\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Action function $q_{\\pi}(s, a)$ reports the value for action $a$ at state $s$ using the policy $\\pi$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy is displayed along with the one-hot encoded version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions)\n",
    "print(policy_one_hot(actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the optimal policy for blackjack found by Monte Carlo exploring starts; blue represents the white represents a stick:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to determine a policy; this is because we did not specify the appropriate number of episodes. We can plot out the action function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>About the Authors:</b> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol type=\"1\">\n",
    "    <li>Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction[</li>\n",
    "    <li><a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\">blackjack openai Gym</a></li>\n",
    "   \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
