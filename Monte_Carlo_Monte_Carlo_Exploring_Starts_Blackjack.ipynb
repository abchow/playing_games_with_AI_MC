{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Monte Carlo Exploring Starts: Black Jack</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will use Perform Exploring Starts on the BlackJack environment on open AI gym </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#UF\">Utility Functions and Imports</a></li>\n",
    "    <li><a href=\"#F\">Monte Carlo Exploring Starts Function</a></li>\n",
    "    <li><a href=\"#FE\">Monte Carlo Prediction Experiments </a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda seaborn\n",
    " \n",
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"UF\">Utility Functions and Imports </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackjackutility import get_total,game_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is used to plot the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(V):\n",
    "    \"\"\"\n",
    "    plot the estimated value function for blackjack \n",
    "    Returns:  void plots value function \n",
    "    Args:\n",
    "    V: a dictionary of estimated values for blackjack \n",
    "    \"\"\"\n",
    "    #range of player score  \n",
    "    player=[state[0]  for state in  V.keys()]\n",
    "    max_player=max(player)\n",
    "    min_player=min(player)\n",
    "    player_range=np.arange(min_player, 22, 1)\n",
    "    #range of dealer score      \n",
    "    dealer=[state[1]  for state in  V.keys()]\n",
    "    max_dealer=max(dealer)\n",
    "    min_dealer=min(dealer)\n",
    "    dealer_range=np.arange(min_dealer, 11, 1)\n",
    "    #empty array for the value function, first access in the players score second  is the dealer, third is if  there  is an ace    \n",
    "    V_plot=np.zeros((21-min_player+1,max_dealer-min_dealer+1,2))\n",
    "    #create a mesh grid for plotting \n",
    "    X,Y = np.meshgrid(dealer_range,player_range)\n",
    "\n",
    "    #populate an array  of values for different  scores not including losing scores \n",
    "    for (player,dealer,ace),v in V.items():\n",
    "        if player<=21 and dealer<=21:\n",
    "            V_plot[player-min_player,dealer-min_dealer,(1*ace)]=V[(player,dealer,ace)]\n",
    "\n",
    "    #plot surface        \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, subplot_kw={'projection': '3d'})\n",
    "    ax[0].plot_wireframe(X,Y, V_plot[:,:,0])\n",
    "    ax[0].set_title('no ace')\n",
    "    ax[0].set_xlabel('dealer')\n",
    "    ax[0].set_ylabel('player ')\n",
    "    ax[0].set_zlabel('value function')\n",
    "    ax[1].plot_wireframe(X,Y, V_plot[:,:,1])\n",
    "    ax[1].set_title('no ace')\n",
    "    ax[1].set_xlabel('dealer')\n",
    "    ax[1].set_ylabel('player ')\n",
    "    ax[1].set_zlabel('value function')\n",
    "    ax[1].set_title(' ace')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #plot top view of the surface     \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)   \n",
    "    ax[0].imshow((V_plot[:,:,0]),extent =[1,10,21,4])\n",
    "    ax[0].set_title('no ace')\n",
    "    ax[0].set_xlabel('dealer')\n",
    "    ax[0].set_ylabel('player ')   \n",
    "    im=ax[1].imshow(V_plot[:,:,1],extent =[1,10,21,4])\n",
    "    ax[1].set_title('ace')\n",
    "    ax[1].set_xlabel('dealer')\n",
    "    fig.colorbar(im, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function will plot blackjack policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_blackjack(policy):\n",
    "    \"\"\"\n",
    "    plot the policy for blackjack \n",
    "    Returns:  void plots policy function \n",
    "    Args:\n",
    "    policy: a dictionary of estimated values for blackjack \n",
    "    \"\"\"    \n",
    "    #range of player score \n",
    "    player=[state[0]  for state in  policy.keys()]\n",
    "    max_player=max(player)\n",
    "    min_player=min(player)\n",
    "    #this vale is use in RL book \n",
    "    #min_player=12\n",
    "    player_range=np.arange(min_player, 22, 1)\n",
    "    #range of dealer score      \n",
    "    dealer=[state[1]  for state in policy.keys()]\n",
    "    max_dealer=max(dealer)\n",
    "    min_dealer=min(dealer)\n",
    "    dealer_range=np.arange(min_dealer, 11, 1)\n",
    "    #empty array for the value function, first access in the players score second  is the dealer, third is if  there  is an ace    \n",
    "    policy_plot=np.zeros((21-min_player+1,max_dealer-min_dealer+1,2))\n",
    "    #create a mesh grid for plotting \n",
    "    X,Y = np.meshgrid(dealer_range,player_range)\n",
    "    \n",
    "    \n",
    "    #populate an array  of values for different  policy not including losing states above 21 \n",
    "    for (player,dealer,ace),v in policy.items():\n",
    "        if player<=21 and dealer<=10 and player>=min_player:\n",
    "            policy_plot[player-min_player,dealer-min_dealer,(1*ace)]=policy[(player,dealer,ace)]\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)   \n",
    "    ax[0].imshow((policy_plot[:,:,0]),cmap=plt.get_cmap('GnBu', 2),extent =[1,10,21,4])\n",
    "    ax[0].set_title('no ace')\n",
    "    ax[0].set_xlabel('dealer')\n",
    "    ax[0].set_ylabel('player ') \n",
    "    \n",
    "\n",
    "    ax[1].set_title('ace')\n",
    "    ax[1].set_xlabel('dealer')\n",
    "    im=ax[1].imshow(policy_plot[:,:,1],extent =[1,10,21,4],cmap=plt.get_cmap('GnBu', 2))\n",
    "    fig.colorbar(im, ax=ax[1],ticks=[0 , 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the average number of wins for a game of blackjack given a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_wins(environment,policy=None,episodes=10):\n",
    "    \"\"\"\n",
    "    This function calculates the average number of wins for a game of blackjack given a policy.\n",
    "    If no policy is provided a random policy is selected.\n",
    "    Returns: average_wins: the average number of wins \n",
    "    std_wins: the average number of wins \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    policy:policy for blackjack if none a random  action will be selected \n",
    "    episodes: number of episodes \n",
    "    \"\"\"\n",
    "\n",
    "    win_loss=np.zeros(episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state=environment.reset()\n",
    "        done=False\n",
    "\n",
    "        while (not(done)):\n",
    "            if policy and isinstance(policy[state],np.int64):\n",
    "                 \n",
    "                action=policy[state]\n",
    "                \n",
    "            else:\n",
    "                action=environment.action_space.sample()\n",
    "\n",
    "            state,reward,done,info=environment.step(action)\n",
    "        result=game_result(environment,state,show=False)\n",
    "        if reward==1:\n",
    "            win_loss[episode]=1\n",
    "        else:\n",
    "            win_loss[episode]=0  \n",
    "\n",
    "        \n",
    "    average_wins=win_loss.mean()\n",
    "    std_win=win_loss.std()/np.sqrt(episodes)\n",
    "\n",
    "    return average_wins ,std_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 id=\"F\">Exploring Starts Function </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function completes exploring starts and returns the policy, value function and action function. The input parameters are the discount factor, the number of episodes and the open AI gym objects.  The class also Specifies if first-visit  and every-visit method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_ES( environment,N_episodes=100000, discount_factor=1,first_visit=True,theta=0.0001):\n",
    "    \"\"\"\n",
    "    plot the policy for blackjack \n",
    "    Returns:  \n",
    "    policy: a dictionary of estimated policy for blackjack \n",
    "    V: a dictionary of estimated values for blackjack \n",
    "    Q: a dictionary of estimated action function\n",
    "    theta:stoping threshold \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    first_visit: select first-visit MC (Ture) and every-visit MC (False)\n",
    "    DELTA: list of deltas for each episode \n",
    "    \"\"\"  \n",
    "    #a dictionary of estimated values for blackjack \n",
    "    V=defaultdict(float)\n",
    "    #a dictionary of estimated action function for blackjack\n",
    "    Q=defaultdict(float)\n",
    "    # number of visits to the action function \n",
    "    NumberVisitsValue= defaultdict(float)\n",
    "    # visits to action function\n",
    "    NumberVisits= defaultdict(float)\n",
    "    #dictionary  for policy \n",
    "    policy=defaultdict(float) \n",
    "    #number  of actions \n",
    "    number_actions=environment.action_space.n\n",
    "    #list of max difference between  value functions per  iteration \n",
    "    DELTA=[]\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "        #max difference between  value functions\n",
    "        delta=0\n",
    "        #list that stores each state and reward for each episode     \n",
    "        episode=[]\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        state=environment.reset()   \n",
    "        #random action for the first state \n",
    "        action=np.random.randint(2)\n",
    "        #reward for the first state (only for frozen lake) \n",
    "        reward=0.0\n",
    "        #flag for end of episodes  \n",
    "        done=False\n",
    "        #action for the first state \n",
    "        action=np.random.randint(number_actions)\n",
    "        #append firt state, reward and action\n",
    "        episode.append({'state':state , 'reward':reward,'action':action})\n",
    "        #Past states for signal visit  Monte Carlo \n",
    "        state_action=[(state,action)]\n",
    "        #enumerate for each episode \n",
    "        while not(done):\n",
    "\n",
    "                #take action and find next state, reward and check if the episode is  done (True)\n",
    "                (state, reward, done, prob) = environment.step(action)\n",
    "\n",
    "                #check if a policy for the state exists  \n",
    "                if isinstance(policy[state],np.int64):\n",
    "                    #obtain action from policy\n",
    "                    action=int(policy[state])\n",
    "\n",
    "                else:\n",
    "                     #if no policy for the state exists  select a random  action  \n",
    "                    action=np.random.randint(number_actions)\n",
    "                #add state reward and action to list \n",
    "                episode.append({'state':state , 'reward':reward,'action':action})\n",
    "                #add  states action this is for fist visit only \n",
    "                state_action.append((state,action))\n",
    "         #reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "        #append the state-action pairs to a list \n",
    "        state_action.reverse()\n",
    "\n",
    "\n",
    "        #determine the return\n",
    "        G=0\n",
    "\n",
    "        for t,step in enumerate(episode):\n",
    "\n",
    "                #check flag for first visit\n",
    "                G=discount_factor*G+step['reward']\n",
    "                #check flag for first visit\n",
    "                if first_visit:\n",
    "                    #check if the state has been visited before \n",
    "                    if (step['state'],step['action']) not in set(state_action[t+1:]): \n",
    "\n",
    "                        #increment counter for action \n",
    "                        NumberVisits[step['state'],step['action']]+=1\n",
    "                        #increment counter for value function \n",
    "                        NumberVisitsValue[step['state']]+=1\n",
    "                        #if the action function value  does not exist, create an array  to store them \n",
    "                        if not(isinstance(Q[step['state']],np.ndarray) ):\n",
    "                            Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n",
    "                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                        \n",
    "                        # record the old value of the value function \n",
    "\n",
    "                        v=V[step['state']]\n",
    "                        \n",
    "                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n",
    "                        #update the policy to select the action fuciton argment with the largest value \n",
    "                        policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        #find max difference between all value functions per  iteration \n",
    "                        delta=max(delta,abs(v-V[step['state']]))\n",
    "\n",
    "\n",
    "                else:\n",
    "                         #increment counter for action \n",
    "                        NumberVisits[step['state'],step['action']]+=1\n",
    "                        #increment counter for value function \n",
    "                        NumberVisitsValue[step['state']]+=1\n",
    "                        #if the action function value  does not exist, create an array  to store them \n",
    "                        if not(isinstance(Q[step['state']],np.ndarray) ):\n",
    "                            Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n",
    "                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                        v=V[step['state']]\n",
    "                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n",
    "                        ##update the policy to select the action fuciton argment with the largest value \n",
    "                        policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        #find max difference between all value functions per  iteration \n",
    "                        delta=max(delta,abs(v-V[step['state']]))\n",
    "            \n",
    "        DELTA.append(delta)\n",
    "        if delta<theta:\n",
    "            break\n",
    "\n",
    "    return policy, V, Q,DELTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 id=\"FE\">Monte Carlo Prediction Experiments  </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s  run different iterations of exploring starts and see how different  parameters such as number of episodes   and discount factor effects performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an openAI gym blackjack enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment= gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we use a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average ,std_win=average_wins(environment,episodes=10000)\n",
    "print(\"average number of wins\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Approximately 28 % of the game are won "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform exploring starts with 1000 episodes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " policy, V, Q,DELTA= monte_carlo_ES( environment,N_episodes=1000, discount_factor=1,first_visit=True,theta=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot delta for each  episode  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(DELTA)\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"delt \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the optimal policy for blackjack found by Monte Carlo exploring starts; blue represents the white represents a stick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_blackjack(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to determine a policy; this is because we did not specify the appropriate number of episodes. We can plot out the action function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a  general trend, as the score of the player increases the value function takes on higher values.  Let see the average result of playing one ten thousand games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "print(\"average wins:\",average,std_win )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An almost 10% improvement from a random policy. Now Lets perform exploring starts with 500000 episodes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, V, Q,DELTA = monte_carlo_ES( environment,N_episodes=500000, discount_factor=1,first_visit=True,theta=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot delta for each  episode  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(DELTA)\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"delt \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy for blackjack. If the agent has no ace, the higher the dealer is showing, the more likely the agent is to hit, the exception is if the dealer has an ace. If the agent has an ace, the strategy is different. The agent will stick if the sum of their cards is over 11 and, for the most part, hold the player's sum is over 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_blackjack(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot the value function,  as the agent or player score  reaches 21 or the dealer score is smaller the larger the value becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see as the accuracy is now approximately 43% a 15 % improvement from a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "print(\"average wins:\",average,std_win )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the number of episodes change the number of wins.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [] \n",
    "episodes = []\n",
    "\n",
    "#policy, V, Q, DELTA = monte_carlo_ES( environment,N_episodes=500000, discount_factor=1,first_visit=True,theta=0)\n",
    "\n",
    "for n_episode in [1,50,100,500,1000,5000,10000,50000,100000,500000]:\n",
    "    print(\"n_episode: \", str(n_episode))\n",
    "    policy, V, Q, DELTA = monte_carlo_ES( environment,N_episodes=n_episode, discount_factor=1,first_visit=True, theta = 0)  \n",
    "    average ,std_win = average_wins(environment,policy,episodes=10000)\n",
    "    print(\"n_episode: \", str(n_episode), \" average: \", str(average))\n",
    "    accuracy.append(average)\n",
    "    episodes.append(n_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that over 1000000 episodes the effect is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episodes,accuracy)\n",
    "plt.title(\"Number of wins vs number of episodes \")\n",
    "plt.ylabel('percentage of wins')\n",
    "plt.xlabel('number of episodes ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount factor changes the number of wins.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[] \n",
    "discounts=[]\n",
    "\n",
    "for discount in [0,0.01,0.1,0.5,1.0]:\n",
    "    policy, V, Q, delta = monte_carlo_ES( environment,N_episodes=100000, discount_factor=discount,first_visit=True, theta=0)  \n",
    "    average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "    print(\"discount: \", str(discount), \" average: \", str(average))\n",
    "    discounts.append(discount)\n",
    "    accuracy.append(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(discounts,accuracy)\n",
    "plt.title(\"percentage of wins vs discount factor \")\n",
    "plt.ylabel('percentage of wins')\n",
    "plt.xlabel('discount factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see as the Discount factor  increases as the percentage  of wins increase,  the rate of increase begins to slow down when the discount  value is  0.6.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
    "<h1> Question: </h1>\n",
    "\n",
    "<b> \n",
    "   \n",
    " Use the function  <code>monte_carlo_ES Monte Carlo Exploring Starts using every-visit MC, use <code>500000</code> discount factor of 1,  Determine the number of wins  after 1000 episodes.\n",
    "?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- The answer is below:\n",
    "\n",
    "policy, V, Q= monte_carlo_ES( environment,N_episodes=500000, discount_factor=1,first_visit=False) \n",
    "plot_policy_blackjack(policy)\n",
    "plot_value_function(V)\n",
    "average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "print(\"average wins:\",average )\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>About the Authors:</b> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol type=\"1\">\n",
    "    <li>Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction[</li>\n",
    "    <li><a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\">blackjack openai Gym</a></li>\n",
    "   \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
