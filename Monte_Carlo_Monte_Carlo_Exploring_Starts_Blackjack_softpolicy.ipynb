{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Monte Carlo Control without Exploring Starts: $\\epsilon$ (epsilon)-greedy policies</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will use Perform Monte Carlo Control without Exploring Starts using $\\epsilon$ (epsilon)-greedy policies on the BlackJack environment on open AI gym </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#UF\">Utility Functions and Imports</a></li>\n",
    "    <li><a href=\"#F\">Monte Carlo Control without Exploring Starts</a></li>\n",
    "    <li><a href=\"#FE\">Monte Carlo Prediction Experiments </a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda seaborn\n",
    " \n",
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"UF\">Utility Functions and Imports </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackjackutility import get_total,game_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is used to plot the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_function(V):\n",
    "    \"\"\"\n",
    "    plot the estimated value function for blackjack \n",
    "    Returns:  void plots value function \n",
    "    Args:\n",
    "    V: a dictionary of estimated values for blackjack \n",
    "    \"\"\"\n",
    "    #range of player score  \n",
    "    player=[state[0]  for state in  V.keys()]\n",
    "    max_player=max(player)\n",
    "    min_player=min(player)\n",
    "    player_range=np.arange(min_player, 22, 1)\n",
    "    #range of dealer score      \n",
    "    dealer=[state[1]  for state in  V.keys()]\n",
    "    max_dealer=max(dealer)\n",
    "    min_dealer=min(dealer)\n",
    "    dealer_range=np.arange(min_dealer, 11, 1)\n",
    "    #empty array for the value function, first access in the players score second  is the dealer, third is if  there  is an ace    \n",
    "    V_plot=np.zeros((21-min_player+1,max_dealer-min_dealer+1,2))\n",
    "    #create a mesh grid for plotting \n",
    "    X,Y = np.meshgrid(dealer_range,player_range)\n",
    "\n",
    "    #populate an array  of values for different  scores not including losing scores \n",
    "    for (player,dealer,ace),v in V.items():\n",
    "        if player<=21 and dealer<=21:\n",
    "            V_plot[player-min_player,dealer-min_dealer,(1*ace)]=V[(player,dealer,ace)]\n",
    "\n",
    "    #plot surface        \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, subplot_kw={'projection': '3d'})\n",
    "    ax[0].plot_wireframe(X,Y, V_plot[:,:,0])\n",
    "    ax[0].set_title('no ace')\n",
    "    ax[0].set_xlabel('dealer')\n",
    "    ax[0].set_ylabel('player ')\n",
    "    ax[0].set_zlabel('value function')\n",
    "    ax[1].plot_wireframe(X,Y, V_plot[:,:,1])\n",
    "    ax[1].set_title('no ace')\n",
    "    ax[1].set_xlabel('dealer')\n",
    "    ax[1].set_ylabel('player ')\n",
    "    ax[1].set_zlabel('value function')\n",
    "    ax[1].set_title(' ace')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #plot top view of the surface     \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)   \n",
    "    ax[0].imshow((V_plot[:,:,0]),extent =[1,10,21,4])\n",
    "    ax[0].set_title('no ace')\n",
    "    ax[0].set_xlabel('dealer')\n",
    "    ax[0].set_ylabel('player ')   \n",
    "    im=ax[1].imshow(V_plot[:,:,1],extent =[1,10,21,4])\n",
    "    ax[1].set_title('ace')\n",
    "    ax[1].set_xlabel('dealer')\n",
    "    fig.colorbar(im, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function will plot blackjack policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_blackjack(policy):\n",
    "    \"\"\"\n",
    "    plot the policy for blackjack \n",
    "    Returns:  void plots policy function \n",
    "    Args:\n",
    "    policy: a dictionary of estimated values for blackjack \n",
    "    \"\"\"    \n",
    "    #range of player score \n",
    "    player=[state[0]  for state in  policy.keys()]\n",
    "    max_player=max(player)\n",
    "    min_player=min(player)\n",
    "    #this vale is use in RL book \n",
    "    #min_player=12\n",
    "    player_range=np.arange(min_player, 22, 1)\n",
    "    #range of dealer score      \n",
    "    dealer=[state[1]  for state in policy.keys()]\n",
    "    max_dealer=max(dealer)\n",
    "    min_dealer=min(dealer)\n",
    "    dealer_range=np.arange(min_dealer, 11, 1)\n",
    "    #empty array for the value function, first access in the players score second  is the dealer, third is if  there  is an ace    \n",
    "    policy_plot=np.zeros((21-min_player+1,max_dealer-min_dealer+1,2))\n",
    "    #create a mesh grid for plotting \n",
    "    X,Y = np.meshgrid(dealer_range,player_range)\n",
    "    \n",
    "    \n",
    "    #populate an array  of values for different  policy not including losing states above 21 \n",
    "    for (player,dealer,ace),v in policy.items():\n",
    "        if player<=21 and dealer<=10 and player>=min_player:\n",
    "            policy_plot[player-min_player,dealer-min_dealer,(1*ace)]=policy[(player,dealer,ace)]\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)   \n",
    "    ax[0].imshow((policy_plot[:,:,0]),cmap=plt.get_cmap('GnBu', 2),extent =[1,10,21,4])\n",
    "    ax[0].set_title('no ace')\n",
    "    ax[0].set_xlabel('dealer')\n",
    "    ax[0].set_ylabel('player ') \n",
    "    \n",
    "\n",
    "    ax[1].set_title('ace')\n",
    "    ax[1].set_xlabel('dealer')\n",
    "    im=ax[1].imshow(policy_plot[:,:,1],extent =[1,10,21,4],cmap=plt.get_cmap('GnBu', 2))\n",
    "    fig.colorbar(im, ax=ax[1],ticks=[0 , 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the average number of wins for a game of blackjack given a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_wins(environment,policy=None,episodes=10):\n",
    "    \"\"\"\n",
    "    This function calculates the average number of wins for a game of blackjack given a policy.\n",
    "    If no policy is provided a random policy is selected.\n",
    "    Returns: average_wins: the average number of wins \n",
    "    std_wins: the average number of wins \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    policy:policy for blackjack if none a random  action will be selected \n",
    "    episodes: number of episodes \n",
    "    \"\"\"\n",
    "\n",
    "    win_loss=np.zeros(episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state=environment.reset()\n",
    "        done=False\n",
    "\n",
    "        while (not(done)):\n",
    "            if policy and isinstance(policy[state],np.int64):\n",
    "                 \n",
    "                action=policy[state]\n",
    "                \n",
    "            else:\n",
    "                action=environment.action_space.sample()\n",
    "\n",
    "            state,reward,done,info=environment.step(action)\n",
    "        result=game_result(environment,state,show=False)\n",
    "        if reward==1:\n",
    "            win_loss[episode]=1\n",
    "        else:\n",
    "            win_loss[episode]=0  \n",
    "\n",
    "        \n",
    "    average_wins=win_loss.mean()\n",
    "    std_win=win_loss.std()/np.sqrt(episodes)\n",
    "\n",
    "    return average_wins ,std_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 id=\"F\">Exploring Starts Function </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will select the maximum argument of its input,  with the probability of epsilon. Otherwise, it will randomly select the non greedy action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(z,epsilon=0.1):\n",
    "    \"\"\"\n",
    "    This function will select the maximum argument of its input, with the probability of epsilon. Otherwise, it will randomly select the non greedy action\n",
    "    Returns:\n",
    "    Args:\n",
    "    z:numpy array representing the action function \n",
    "    epsilon: probability of selecting the non-greedy action \n",
    "    policy:policy for blackjack if none a random  action will be selected \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if np.random.rand(1)<1-epsilon:\n",
    "    \n",
    "        argmax_=np.random.choice(np.where(z==z.max())[0])\n",
    "    else:\n",
    "        argmax_=np.random.randint(len(z))\n",
    "\n",
    "    return argmax_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array data has its maximum value at index 0. We set epsilon to 0.75; as a result, it will only select the maximum value  25% of the time. Let's run the experiment 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([10.0,1,1,3])\n",
    "for i in range(10):\n",
    "    print(\"{}=argmax(data)\".format(argmax(data,epsilon=0.75)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see most of the samples are not the index with the largest values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines the optimal policy using the  $\\epsilon$ (epsilon)-greedy method. The input parameters are the discount factor, the number of episodes, epsilon value  and the open AI gym objects.  The class also Specifies if first-visit  and every-visit method. The output is the policy,value function and action function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_ES( environment,N_episodes=100000,epsilon=0.1, discount_factor=1,first_visit=True,theta=0.0001):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy using the epsilon greedy method. The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects. The class also Specifies if first-visit and every-visit method. The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy: a dictionary of estimated policy for blackjack \n",
    "    V: a dictionary of estimated values for blackjack \n",
    "    Q: a dictionary of estimated action function\n",
    "    DELTA: list of deltas for each episode\n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    first_visit: select first-visit MC (Ture) and every-visit MC (False)\n",
    "    \"\"\"  \n",
    "    #a dictionary of estimated values for blackjack \n",
    "    V=defaultdict(float)\n",
    "    #a dictionary of estimated action function for blackjack\n",
    "    Q=defaultdict(float)\n",
    "    # number of visits to the action function \n",
    "    NumberVisitsValue= defaultdict(float)\n",
    "    # visits to action function\n",
    "    NumberVisits= defaultdict(float)\n",
    "    #dictionary  for policy \n",
    "    policy=defaultdict(float) \n",
    "    #number  of actions \n",
    "    number_actions=environment.action_space.n\n",
    "    DELTA=[] \n",
    "\n",
    "    for i in range(N_episodes):\n",
    "        #difference between  current  value function estimate and  previous value function estimate \n",
    "        delta=0\n",
    "        \n",
    "        #list that stores each state and reward for each episode     \n",
    "        episode=[]\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        state=environment.reset()   \n",
    "        #random action for the first state \n",
    "        action=np.random.randint(2)\n",
    "        #reward for the first state (only for frozen lake) \n",
    "        reward=0.0\n",
    "        #flag for end of episodes  \n",
    "        done=False\n",
    "        \n",
    "         #check if a policy for the state exists  \n",
    "        if isinstance(policy[state],np.int64):\n",
    "            #obtain action from policy\n",
    "            action=int(policy[state])\n",
    "\n",
    "        else:\n",
    "            #if no policy for the state exists  select a random  action  \n",
    "            action=np.random.randint(number_actions)\n",
    "        \n",
    "        \n",
    "        #action for the first state \n",
    "        action=np.random.randint(number_actions)\n",
    "        #append firt state, reward and action\n",
    "        episode.append({'state':state , 'reward':reward,'action':action})\n",
    "        #Past states for signal visit  Monte Carlo \n",
    "        state_action=[(state,action)]\n",
    "        #enumerate for each episode \n",
    "        while not(done):\n",
    "\n",
    "                #take action and find next state, reward and check if the episode is  done (True)\n",
    "                (state, reward, done, prob) = environment.step(action)\n",
    "\n",
    "                #check if a policy for the state exists  \n",
    "                if isinstance(policy[state],np.int64):\n",
    "                    #obtain action from policy\n",
    "                    action=int(policy[state])\n",
    "\n",
    "                else:\n",
    "                     #if no policy for the state exists  select a random  action  \n",
    "                    action=np.random.randint(number_actions)\n",
    "                #add state reward and action to list \n",
    "                episode.append({'state':state , 'reward':reward,'action':action})\n",
    "                #add  states action this is for fist visit only \n",
    "                state_action.append((state,action))\n",
    "         #reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "        #append the state-action pairs to a list \n",
    "        state_action.reverse()\n",
    "\n",
    "\n",
    "        #determine the return\n",
    "        G=0\n",
    "\n",
    "        for t,step in enumerate(episode):\n",
    "\n",
    "                #check flag for fist visit\n",
    "                G=discount_factor*G+step['reward']\n",
    "                #check flag for fist visit\n",
    "                if first_visit:\n",
    "                    #check if the state has been visited before \n",
    "                    if (step['state'],step['action']) not in set(state_action[t+1:]): \n",
    "\n",
    "                        #increment counter for action \n",
    "                        NumberVisits[step['state'],step['action']]+=1\n",
    "                        #increment counter for value function \n",
    "                        NumberVisitsValue[step['state']]+=1\n",
    "                        #if the action function value  does not exist, create an array  to store them \n",
    "                        if not(isinstance(Q[step['state']],np.ndarray) ):\n",
    "                            Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n",
    "                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                        #previous value function estimate \n",
    "                        v=V[step['state']]\n",
    "                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n",
    "                        ##update the policy to select the action fuciton argment with the largest value or randomly select a different action \n",
    "                        policy[step['state']]=argmax(Q[step['state']],epsilon=epsilon)\n",
    "                        #difference between  current  value function estimate and  previous value function estimate \n",
    "                        delta=max(delta,abs(v-V[step['state']]))\n",
    "\n",
    "\n",
    "                else:\n",
    "                         #increment counter for action \n",
    "                        NumberVisits[step['state'],step['action']]+=1\n",
    "                        #increment counter for value function \n",
    "                        NumberVisitsValue[step['state']]+=1\n",
    "                        #if the action function value  does not exist, create an array  to store them \n",
    "                        if not(isinstance(Q[step['state']],np.ndarray) ):\n",
    "                            Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                        #calculate mean of action function Q Value functions V using the  recursive definition of mean \n",
    "                        Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                        #previous value function estimate \n",
    "                        v=V[step['state']]\n",
    "                        V[step['state']]=V[step['state']]+(NumberVisitsValue[step['state']]**-1)*(G-V[step['state']])\n",
    "                        ##update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "                        policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        #difference between  current  value function estimate and  previous value function estimate \n",
    "                        delta=max(delta,abs(v-V[step['state']]))\n",
    "        #Delta for each episode  \n",
    "        DELTA.append(delta)\n",
    "        if delta<theta:\n",
    "            break\n",
    "\n",
    "\n",
    "    return policy, V, Q,DELTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 id=\"FE\">Monte Carlo Prediction Experiments  </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s  run different iterations of and see how different  parameters such as number of episodes  and discount factor effects performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an openAI gym blackjack enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment= gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we use a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average ,std_win=average_wins(environment,episodes=10000)\n",
    "\n",
    "\n",
    "print(\"average wins:\",average,std_win )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Approximately 28 % of the game are wone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform  $\\epsilon$ -greedy method  with 1000 episodes and  $\\epsilon$=0.1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " policy, V, Q,DELTA= monte_carlo_ES( environment,N_episodes=1000,epsilon=0.1, discount_factor=1,first_visit=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the optimal policy for blackjack found by Monte Carlo exploring starts; blue represents the white represents a stick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_blackjack(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to determine a policy; this is because we did not specify the appropriate number of episodes. We can plot out the action function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a  general trend, as the score of the player increases the value function takes on higher values.  Let see the average result of playing one ten thousand games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "print(\"average wins:\",average,std_win )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An almost 10% improvement from a random policy. Now Lets perform exploring starts with 500000 episodes and  $\\epsilon$=0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, V, Q,DELTA= monte_carlo_ES( environment,epsilon=0.1,N_episodes=500000, discount_factor=1,first_visit=True,theta=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot delta for each episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(DELTA)\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"delt \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy for blackjack. If the agent has no ace, the higher the dealer is showing, the more likely the agent is to hit, the exception is if the dealer has an ace. If the agent has an ace, the strategy is different. The agent will stick if the sum of there cards is over 11 and, for the most part, hold the player's sum is over 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_blackjack(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot the value function,  as the agent or player score  reaches 21 or the dealer score is smaller the larger the value becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see as the accuracy is now approximately 40% a 15 % improvement from a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "print(\"average wins:\",average,std_win )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how different  values for  $\\epsilon$  number of wins.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[] \n",
    "epsilons =[]\n",
    "\n",
    "for epsilon in [0,0.001,0.002,0.004,0.008,0.01,0.02,0.04,0.08,0.1,0.2,0.4,0.8,1.0]:\n",
    "    policy, V, Q,DELTA= monte_carlo_ES( environment,epsilon=epsilon ,N_episodes=200000, discount_factor=1,first_visit=True, theta=0)  \n",
    "    average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "    accuracy.append(average)\n",
    "    epsilons.append(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an epsilon value of 0.008 has the best results. As epsilon increases, the number of wins decreases until it identical to a  random policy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilons,accuracy)\n",
    "plt.title(\"Number of wins vs epsilon\")\n",
    "plt.ylabel('percentage of wins')\n",
    "plt.xlabel('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how the discount factor changes the number of wins.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[] \n",
    "discounts=[]\n",
    "\n",
    "for discount in [0,0.01,0.1,0.5,1.0]:\n",
    "    policy, V, Q,DELTA= monte_carlo_ES( environment,N_episodes=100000, epsilon=0.02, discount_factor=discount,first_visit=True,theta=0)  \n",
    "    average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "    discounts.append(discount)\n",
    "    accuracy.append(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount factor and percentage of wins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(discounts,accuracy)\n",
    "plt.title(\"percentage of wins vs discount factor \")\n",
    "plt.ylabel('percentage of wins')\n",
    "plt.xlabel('discount factor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see as the Discount factor  increases as the percentage  of wins increase,  the rate of increase begins to slow down when the discount  value is  0.6.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
    "<h1> Question: </h1>\n",
    "\n",
    "<b> \n",
    "   \n",
    " Use the fuciton  <code>monte_carlo_ES Monte Carlo Exploring Starts using every-visit MC, use <code>500000</code> discount factor of 1 and epsilon value of 0.1,  Determine the number of wins  after 1000 episodes.\n",
    "?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- The answer is below:\n",
    "\n",
    "policy, V, Q,DELTA= monte_carlo_ES( environment,N_episodes=500000, discount_factor=1,first_visit=False,theta=0) \n",
    "plot_policy_blackjack(policy)\n",
    "plot_value_function(V)\n",
    "average ,std_win=average_wins(environment,policy,episodes=10000)\n",
    "print(\"average wins:\",average )\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>About the Authors:</b> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol type=\"1\">\n",
    "    <li>Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction</li>\n",
    "    <li><a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\">blackjack openai Gym</a></li>\n",
    "   \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
